{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"},{"sourceId":12627018,"sourceType":"datasetVersion","datasetId":7978352}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Objective: \n\nBuild a GAN that takes in real photos and recreates them in a Monet-like style painting.","metadata":{}},{"cell_type":"markdown","source":"Modeling Description: \n\nGenerative adversarial networks (GAN) are used in deep learning and represents the application of using two types of neural networks: a generator and a discriminator.\n\nThe generator creates fake data while the discriminator tries to identify that data as fake. If the discriminator is successful, the generator is penalized, it adjusts the parameters used to generate the fake data, and it re-generates the fake data for the discriminator to assess again. \n\nIf the generator is successful, the discriminator is penalized and it learns how to identify the new fake data. \n\nBoth networks are in competition with each other and repeats the cycle until the discrimanator is unable to identify the real vs fake data.","metadata":{}},{"cell_type":"markdown","source":"Data Description: \n\nThe dataset includes 300 Monet paintings to train the model, and 7000 photos to convert to a Monet-style like painting.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport os\nfrom PIL import Image\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T02:17:25.905915Z","iopub.execute_input":"2025-07-31T02:17:25.906395Z","iopub.status.idle":"2025-07-31T02:17:25.912757Z","shell.execute_reply.started":"2025-07-31T02:17:25.906361Z","shell.execute_reply":"2025-07-31T02:17:25.911753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bringing in monets\nmonets = '/kaggle/input/gan-getting-started/monet_jpg/'\nmonet_files = [os.path.join(monets,f) for f in os.listdir(monets) if os.path.isfile(os.path.join(monets,f))]\nmonet_files_cnv = [np.array(Image.open(f)) for f in monet_files]\nprint('Monet paintings: ', len(os.listdir(monets)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#bringing in photos\nphotos = '/kaggle/input/gan-getting-started/photo_jpg/'\nphoto_files = [os.path.join(photos,f) for f in os.listdir(photos) if os.path.isfile(os.path.join(photos,f))]\nphoto_files_cnv = [np.array(Image.open(f)) for f in photo_files]\nprint('Photos: ', len(os.listdir(photos)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Viewing the images\nplt.subplot(121)\nplt.title('Monet: 17')\nplt.imshow(mpimg.imread(monet_files[17]))\n\n\nplt.subplot(122)\nplt.title('Photo: 17')\nplt.imshow(mpimg.imread(photo_files[17]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Paths for tfrec files\ntf_monet = tf.io.gfile.glob(str('/kaggle/input/gan-getting-started/monet_tfrec/*.tfrec'))\ntf_photo = tf.io.gfile.glob(str('/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#helper functions to decode and read tfrec\nsizing = [256,256]\n\ndef decode(file):\n    file = tf.image.decode_jpeg(file,channels=3)\n    file = (tf.cast(file,tf.float32)/127.5) - 1\n    file = tf.reshape(file, [*sizing,3])\n    return file\n\ndef read_tfrec(record):\n    tfrec_form = {\n        \"name\": tf.io.FixedLenFeature([], tf.string),\n        \"pic\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([],tf.string)\n    }\n    record = tf.io.parse_single_example(record, tfrec_form)\n    pic = decode(record['pic'])\n    return pic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pulling in tfrec files\ndef load_tfrec(files, label=True, order=False):\n    data = tf.data.TFRecordDataset(files)\n    data = data.map(read_tfrec)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"photo_data = load_tfrec(tf_photo, label=True).batch(1)\nmonet_data = load_tfrec(tf_monet, label=True).batch(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Building:\n\nWe need to build the generator and discriminator for this architecture. Both require CNN models and need multiple layers to manipulate the images into the right format.","metadata":{}},{"cell_type":"code","source":"#building the generator\noutput_ch = 3\n\n## helpers for image size layer\ndef down_sample(x, y):\n    init = tf.random_normal_initializer(0,.05)\n    gamma = keras.initializers.RandomNormal(mean=0, stddev=.05)\n    \n    res = keras.Sequential()\n    res.add(layers.Conv2D(x,y,strides=2,padding='same', kernel_initializer=init, use_bias=False))\n    res.add(layers.LeakyReLU())\n\n    return res\n\ndef up_sample(x,y):\n    init = tf.random_normal_initializer(0,.05)\n    gamma = keras.initializers.RandomNormal(mean=0, stddev=.05)\n    \n    res = keras.Sequential()\n    res.add(layers.Conv2DTranspose(x,y,strides=2,padding='same', kernel_initializer=init, use_bias=False))\n    res.add(layers.ReLU())\n\n    return res","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#building the generator\ndef Gen():\n    inputs = layers.Input(shape=[256,256,3])\n\n    downs = [down_sample(64,4),\n             down_sample(128, 4),\n             down_sample(256, 4),\n             down_sample(512, 4),\n             down_sample(512, 4),\n             down_sample(512, 4),\n             down_sample(512, 4),\n             down_sample(512, 4)]\n    ups = [\n        up_sample(512, 4),\n        up_sample(512, 4),\n        up_sample(512, 4),\n        up_sample(512, 4),\n        up_sample(256, 4),\n        up_sample(128, 4),\n        up_sample(64, 4),\n    ]\n    \n    init = tf.random_normal_initializer(0., 0.05)\n    last = layers.Conv2DTranspose(output_ch, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=init,\n                                  activation='tanh')\n\n    fs = inputs\n\n    ls_miss = []\n    for each in downs:\n        fs = each(fs)\n        ls_miss.append(fs)\n\n    ls_miss = reversed(ls_miss[:-1])\n\n    for i,j in zip(ups, ls_miss):\n        fs = i(fs)\n        fs = layers.Concatenate()([fs, j])\n\n    fs = last(fs)\n\n    return keras.Model(inputs=inputs, outputs=fs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#building the discriminator\ndef Dis():\n    init = tf.random_normal_initializer(0,.05)\n    gamma = keras.initializers.RandomNormal(mean=0, stddev=.05)\n\n    inputs = layers.Input(shape=[256,256,3])\n\n    fs = inputs\n\n    downs_a = down_sample(64,4)(fs)\n    downs_b = down_sample(128,4)(downs_a)\n    downs_c = down_sample(256,4)(downs_b)\n\n    pads = layers.ZeroPadding2D()(downs_c)\n    cons = layers.Conv2D(512,4,strides=1, kernel_initializer=init, use_bias=False)(pads)\n\n    last = layers.Conv2D(1,4,strides=1,kernel_initializer=init)(pads)\n\n    return tf.keras.Model(inputs=inputs,outputs=last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Generator and Discriminator Creation\nph_gen = Gen()\nph_dis = Dis()\n\nmn_gen = Gen()\nmn_dis = Dis()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#building the full GAN\nclass CycleGan(keras.Model):\n    def __init__(\n        self,\n        mn_gen,\n        ph_gen,\n        mn_dis,\n        ph_dis,\n        lambdas=5,\n    ):\n        super(CycleGan, self).__init__()\n        self.mngen = mn_gen\n        self.phgen = ph_gen\n        self.mndis = mn_dis\n        self.phdis = ph_dis\n        self.lambda_num = lambdas\n\n    def compile(\n        self,\n        opt_mgen,\n        opt_pgen,\n        opt_mdis,\n        opt_pdis,\n        loss_gn,\n        loss_dis,\n        loss_cycle,\n        loss_id\n    ):\n        super(CycleGan, self).compile()\n        self.opt_mgen = opt_mgen\n        self.opt_pgen = opt_pgen\n        self.opt_mdis = opt_mdis\n        self.opt_pdis = opt_pdis\n        self.loss_gn = loss_gn\n        self.loss_dis = loss_dis\n        self.loss_cycle = loss_cycle\n        self.loss_id = loss_id\n        \n    def call(self, inputs):\n        gens = self.mngen\n       return self.model(inputs)\n        \n    def training(self, data):\n        mn_rl, ph_rl = data\n        \n        with tf.GradientTape(persistent=True) as tp:\n            mn_fk = self.mngen(ph_rl, training=True)\n            cnv_ph = self.phgen(mn_fk, training=True)\n\n            ph_fk = self.p_gen(mn_rl, training=True)\n            cnv_mn = self.m_gen(ph_fk, training=True)\n\n            mnsm = self.m_gen(mn_rl, training=True)\n            phsm = self.p_gen(ph_rl, training=True)\n\n            dis_mn_rl = self.mn_dis(mn_rl, training=True)\n            dis_ph_rl = self.ph_dis(ph_rl, training=True)\n\n            dis_mn_fk = self.mn_dis(mn_fk, training=True)\n            dis_ph_fk = self.ph_dis(ph_fk, training=True)\n\n            mn_gen_loss = self.loss_gn(dis_mn_fk)\n            ph_gen_loss = self.loss_gn(dis_ph_fk)\n\n            total_loss = self.loss_cycle(mn_rl, cnv_mn, self.lambda_num) + self.loss_cycle(ph_rl, cnv_ph, self.lambda_num)\n\n            total_mn_gen_loss = mn_gen_loss + total_loss + self.loss_id(mn_rl, mnsm, self.lambda_num)\n            total_ph_gen_loss = ph_gen_loss + total_loss + self.loss_id(ph_rl, phsm, self.lambda_num)\n\n            mn_dis_loss = self.loss_dis(dis_mn_rl, dis_mn_fk)\n            ph_dis_loss = self.loss_dis(dis_ph_rl, dis_ph_fk)\n\n        mn_gen_grads = tp.gradient(total_mn_gen_loss, self.mngen.train)\n        ph_gen_grads = tp.gradient(total_ph_gen_loss, self.phgen.train)\n        mn_dis_grads = tp.gradient(mn_dis_loss, self.mn_dis.train)\n        ph_dis_grads = tp.gradient(ph_dis_loss, self.ph_dis.train)\n\n        self.opt_mgen.apply_gradients(zip(mn_gen_grads, self.mngen.train))\n        self.opt_pgen.apply_gradients(zip(ph_gen_grads, self.phgen.train))\n        self.opt_mdis.apply_gradients(zip(mn_dis_grads, self.mn_dis.train))\n        self.opt_pdis.apply_gradients(zip(ph_dis_grads, self.ph_dis.train))\n        \n        return {\n            \"mn_gen_loss\": total_mn_gen_loss,\n            \"ph_gen_loss\": total_ph_gen_loss,\n            \"mn_dis_loss\": mn_dis_loss,\n            \"ph_dis_loss\": ph_dis_loss\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loss functions for the model\n\ndef loss_id_in(x,y,z):\n    loss = tf.reduce_mean(tf.abs(x-y))\n    return z * 0.5 * loss\n\ndef loss_dis_in(x,y):\n    x_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(x), x)\n    y_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(y), y)\n    total_dis_loss = x_loss + y_loss\n\ndef loss_cycle_in(x, y, z):\n    loss_fl = tf.reduce_mean(tf.abs(x - y))\n    return z * loss_fl\n\ndef loss_gen_in(x):\n    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(x), x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optimizer for model creation\nopt_mgen_in = tf.keras.optimizers.Adam(1e-4, beta_1=0.1)\nopt_pgen_in = tf.keras.optimizers.Adam(1e-4, beta_1=0.1)\n\nopt_mdis_in = tf.keras.optimizers.Adam(1e-4, beta_1=0.1)\nopt_pdis_in = tf.keras.optimizers.Adam(1e-4, beta_1=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Init the GAN Model\ngan_model = CycleGan(mn_gen, ph_gen, mn_dis, ph_dis)\ngan_model.compile(\n    opt_mgen = opt_mgen_in,\n    opt_pgen = opt_pgen_in,\n    opt_mdis = opt_mdis_in,\n    opt_pdis = opt_pdis_in,\n    loss_gn = loss_gen_in,\n    loss_dis = loss_dis_in,\n    loss_cycle = loss_cycle_in,\n    loss_id = loss_id_in)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#training the model on the data\ngan_model.fit(tf.data.Dataset.zip((monet_data,photo_data)),\n              epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Submission Generation","metadata":{}},{"cell_type":"code","source":"i = 1\nfor each in photo_data:\n    pred = monet_generator(each, training=False)[0].numpy()\n    pred = (pred * 127.5 + 127.5).astype(np.uint8)\n    each = Image.fromarray(pred)\n    im.save(\"images/\" + str(i) + \".jpg\")\n    i += 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Building, training and compiling a generatide adversial model is quite complicated. It requires the careful handling of multiple layers, specific configurations on the generator and discriminator and it requires a lot of computational power. However, while complicated and time-intensive, it proved to be a powerful application of deep learning because it yielded images that were fairly acceptable. \n\nIn other contexts, I would experiment more with adding layers or tuning specific layers that are earlier in the process. I would include additional epochs which would increase computation time but might yield better results. I might use more distinct or defined images to see how the model handles images with lots of detail. ","metadata":{}}]}
